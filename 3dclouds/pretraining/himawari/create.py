"""
TACO Build Script

Orchestrates the complete TACO dataset build process.
Reads configuration from dataset/config.py and calls dataset/taco.py.

DO NOT EDIT THIS FILE - Modify files in the dataset/ directory instead.

Usage:
    python dataset/create.py
"""

import re
import shutil
from pathlib import Path

import tacotoolbox
from tacotoolbox import create
from dataset.config import BUILD_CONFIG, PARQUET_CONFIG
from dataset.taco import create_taco
from dataset.metadata import load_contexts


def clean_previous_outputs(output: str):
    """
    Remove previous TACO outputs safely using regex patterns.
    
    Handles:
    - Single file: output.tacozip, output.zip
    - Parts: output_part0001.tacozip, output_part0002.tacozip, ...
    - Groups: output_groupA.tacozip, output_groupB.tacozip, ...
    - FOLDER: output/
    - TacoCat: .tacocat/
    - Docs: index.html, README.md
    """
    output_path = Path(output)
    parent_dir = output_path.parent
    base_stem = output_path.stem

    # Remove suffix if present
    if base_stem.endswith(".tacozip"):
        base_stem = base_stem[:-9]
    elif base_stem.endswith(".zip"):
        base_stem = base_stem[:-4]

    removed = []

    # Single files
    for ext in [".tacozip", ".zip"]:
        single_file = parent_dir / f"{base_stem}{ext}"
        if single_file.exists() and single_file.is_file():
            single_file.unlink()
            removed.append(str(single_file))

    # Parts pattern
    part_pattern = re.compile(rf"^{re.escape(base_stem)}_part\d{{4}}\.(tacozip|zip)$")
    for file in parent_dir.iterdir():
        if file.is_file() and part_pattern.match(file.name):
            file.unlink()
            removed.append(str(file))

    # Group pattern (not _partXXXX)
    groupby_pattern = re.compile(
        rf"^{re.escape(base_stem)}_(?!part\d{{4}}).*\.(tacozip|zip)$"
    )
    for file in parent_dir.iterdir():
        if file.is_file() and groupby_pattern.match(file.name):
            file.unlink()
            removed.append(str(file))

    # FOLDER
    folder_path = parent_dir / base_stem
    if folder_path.exists() and folder_path.is_dir():
        shutil.rmtree(folder_path)
        removed.append(str(folder_path))

    # TacoCat folder
    tacocat_path = parent_dir / ".tacocat"
    if tacocat_path.exists() and tacocat_path.is_dir():
        shutil.rmtree(tacocat_path)
        removed.append(str(tacocat_path))

    # Documentation
    for doc_file in ["index.html", "README.md"]:
        doc_path = parent_dir / doc_file
        if doc_path.exists() and doc_path.is_file():
            doc_path.unlink()
            removed.append(str(doc_path))

    if removed:
        print(f"Cleaned {len(removed)} previous output(s):")
        for item in removed:
            print(f"  - {item}")


def generate_documentation(output: str, config: dict):
    """Generate HTML and Markdown documentation from .tacocat/ or COLLECTION.json."""
    from tacotoolbox import generate_html, generate_markdown
    
    output_path = Path(output)
    parent_dir = output_path.parent
    
    tacocat_dir = parent_dir / ".tacocat"
    
    if tacocat_dir.exists() and tacocat_dir.is_dir():
        input_path = tacocat_dir / "COLLECTION.json"
        if not input_path.exists():
            print(f"\nWARNING: {input_path} not found, skipping documentation generation")
            return
    else:
        input_path = parent_dir / "COLLECTION.json"
        if not input_path.exists():
            print(f"\nWARNING: {input_path} not found, skipping documentation generation")
            return
    
    print(f"\nGenerating documentation from {input_path}...")
    
    # Extract doc config
    theme_color = config.get("theme_color", "#4CAF50")
    dataset_example_path = config.get("dataset_example_path")
    
    try:
        generate_html(
            input=input_path,
            output=parent_dir / "index.html",
            download_base_url=config.get("download_base_url"),
            catalogue_url=config.get("catalogue_url", "https://tacofoundation.github.io/catalogue"),
            theme_color=theme_color,
            dataset_example_path=dataset_example_path,
        )
        print("Generated index.html")
    except Exception as e:
        print(f"Failed to generate HTML: {e}")
    
    try:
        generate_markdown(
            input=input_path,
            output=parent_dir / "README.md",
            dataset_example_path=dataset_example_path,
        )
        print("Generated README.md")
    except Exception as e:
        print(f"Failed to generate Markdown: {e}")
    
    print(f"\nDocumentation generated in {parent_dir}")


def main():
    """
    Build and write TACO dataset.
    
    Process:
    1. Clean previous outputs (if enabled)
    2. Load contexts with optional limit
    3. Build TACO object
    4. Validate schema (if enabled)
    5. Write to disk with create()
    6. Auto-consolidate to .tacocat/ if multiple ZIPs (if enabled)
    7. Generate documentation (if enabled)
    """
    output = BUILD_CONFIG["output"]
    output_format = BUILD_CONFIG["format"]
    split_size = BUILD_CONFIG.get("split_size")
    group_by = BUILD_CONFIG.get("group_by")
    consolidate = BUILD_CONFIG.get("consolidate", True)
    clean_outputs = BUILD_CONFIG.get("clean_previous_outputs", True)
    validate_schema = BUILD_CONFIG.get("validate_schema", True)
    level0_sample_limit = BUILD_CONFIG.get("level0_sample_limit")

    # Enable/disable logging
    tacotoolbox.verbose(True)

    # Step 1: Clean previous outputs
    if clean_outputs:
        print("Checking for previous outputs...")
        clean_previous_outputs(output)

    # Step 2: Load contexts
    print("\nLoading contexts...")
    contexts = load_contexts(limit=level0_sample_limit)
    print(f"Loaded {len(contexts)} contexts")
    
    if level0_sample_limit:
        print(f"(Limited to {level0_sample_limit} for testing)")

    # Step 3: Build TACO object
    print("\nBuilding TACO object...")
    try:
        taco = create_taco(contexts=contexts)
    except Exception as e:
        print(f"\nERROR: Failed to build TACO: {e}")
        raise

    # Step 4: Validate schema
    if validate_schema:
        print("\nValidating schema...")
        try:
            # Check that all samples have consistent schema
            taco.tortilla.export_metadata()
            print("✓ Schema validation passed")
        except Exception as e:
            print(f"✗ Schema validation failed: {e}")
            raise

    # Step 5: Write to disk
    print(f"\nWriting TACO in {output_format.upper()} format to {output}...")
    
    try:
        paths = create(
            taco=taco,
            output=output,
            output_format=output_format,
            split_size=split_size,
            group_by=group_by,
            consolidate=consolidate,
            **PARQUET_CONFIG
        )
    except Exception as e:
        print(f"\nERROR: Failed to create TACO: {e}")
        raise
    
    print(f"\nCreated {len(paths)} file(s)")
    for path in paths:
        print(f"  - {path}")
    
    # Step 6: Generate COLLECTION.json (if single file, not consolidated)
    output_path = Path(output)
    parent_dir = output_path.parent
    tacocat_path = parent_dir / ".tacocat"
    
    if tacocat_path.exists() and tacocat_path.is_dir():
        # Multiple ZIPs consolidated - COLLECTION.json should be inside .tacocat/
        collection_in_tacocat = tacocat_path / "COLLECTION.json"
        if collection_in_tacocat.exists():
            print(f"\nConsolidated metadata in {tacocat_path}")
        else:
            print(f"\nWARNING: Expected {collection_in_tacocat} but not found")
            print("Consolidation may have failed - check warnings above")
    else:
        # Single file - generate COLLECTION.json in parent dir
        print(f"\nGenerating COLLECTION.json in {parent_dir}")
        collection_path = parent_dir / "COLLECTION.json"
        
        # Export COLLECTION from Taco object (exclude tortilla)
        collection_json = taco.model_dump(
            exclude={'tortilla'},
            mode='json'
        )
        
        import json
        with open(collection_path, 'w') as f:
            json.dump(collection_json, f, indent=2, default=str)
        
        print(f"Created {collection_path}")

    # Step 7: Generate documentation
    if BUILD_CONFIG.get("generate_docs", True):
        generate_documentation(output, BUILD_CONFIG)

    print("\n✓ Build completed successfully!")
    print(f"\nDataset: {taco.id} v{taco.dataset_version}")
    print(f"Samples: {len(taco.tortilla.samples)}")
    print(f"Output:  {output}")


if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\n\nBuild interrupted by user")
        exit(1)
    except Exception as e:
        print(f"\n\nBuild failed: {e}")
        exit(1)